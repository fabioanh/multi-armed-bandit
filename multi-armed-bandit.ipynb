{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Classes\n",
    "Definition of basic classes required for the solution of the multi-armed bandit problem. Inspiration and code used from https://github.com/bgalbraith/bandits. The problem explanation can be better follwed in the chapter 2 of the book _“Reinforcement Learning, An Introduction”_ by Sutton and Barto  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspired by https://github.com/bgalbraith/bandits\n",
    "# Some part of the code were taken from this repository \n",
    "# and modified according to requirements of the current project.\n",
    "# License of the project adjusted to be compliant with the used source.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MultiArmedBandit(object):\n",
    "    \"\"\"\n",
    "    A Multi-armed Bandit\n",
    "    \"\"\"\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.action_values = np.zeros(k)\n",
    "        self.optimal = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.action_values = np.zeros(self.k)\n",
    "        self.optimal = 0\n",
    "\n",
    "    def pull(self, action):\n",
    "        return 0, True\n",
    "    \n",
    "class MultiArmedBanditt(object):\n",
    "    \"\"\"\n",
    "    A Multi-armed Bandit with independently customizable arms\n",
    "    \"\"\"\n",
    "    def __init__(self, arms):\n",
    "        self.arms = arms\n",
    "        self.k = len(self.arms)\n",
    "        self.action_values = np.zeros(len(arms))\n",
    "        self.optimal = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.action_values = np.zeros(len(self.arms))\n",
    "        self.optimal = 0\n",
    "        for arm in self.arms:\n",
    "            arm.reset()\n",
    "\n",
    "    def pull(self, action):\n",
    "        return self.arms[action].pull(), True\n",
    "\n",
    "class Arm(object):\n",
    "    \"\"\"\n",
    "    Representation of an arm to be used by a multiarmed bandit.\n",
    "    By default this arm represent a random arm picking its values from a uniform distribution\n",
    "    between 0 and 1.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.count = 0 # counter of the times the arm is pulled\n",
    "        \n",
    "    def pull(self):\n",
    "        self.count += 1\n",
    "        return np.random.uniform();\n",
    "\n",
    "class GaussianArm(Arm):\n",
    "    \"\"\"\n",
    "    Representation of an arm using the values coming from a normal distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, mu=0, sigma=1):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        super(GaussianArm, self).__init__()\n",
    "    \n",
    "    def pull(self):\n",
    "        self.count += 1\n",
    "        return np.random.normal(self.mu, self.sigma)\n",
    "    \n",
    "class GaussianArm2(Arm):\n",
    "    \"\"\"\n",
    "    Representation of an arm using the values coming from a normal distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        self.mean = np.random.normal(self.mu, self.sigma)\n",
    "    \n",
    "    def pull(self):\n",
    "        self.count += 1\n",
    "        return np.random.normal(self.mean)\n",
    "\n",
    "class Policy(object):\n",
    "    \"\"\"\n",
    "    A policy prescribes an action to be taken based on the memory of an agent.\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return 'generic policy'\n",
    "\n",
    "    def choose(self, agent):\n",
    "        return 0\n",
    "\n",
    "\n",
    "class EpsilonGreedyPolicy(Policy):\n",
    "    \"\"\"\n",
    "    The Epsilon-Greedy policy will choose a random action with probability\n",
    "    epsilon and take the best apparent approach with probability 1-epsilon. If\n",
    "    multiple actions are tied for best choice, then a random action from that\n",
    "    subset is selected.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\u03B5-greedy (\\u03B5={})'.format(self.epsilon)\n",
    "\n",
    "    def choose(self, agent):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.choice(len(agent.value_estimates))\n",
    "        else:\n",
    "            action = np.argmax(agent.value_estimates)\n",
    "            check = np.where(agent.value_estimates == action)[0]\n",
    "            if len(check) == 0:\n",
    "                return action\n",
    "            else:\n",
    "                return np.random.choice(check)\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    \"\"\"\n",
    "    The Random policy randomly selects from all available actions with no\n",
    "    consideration to which is apparently best. This can be seen as a special\n",
    "    case of EpsilonGreedy where epsilon = 1 i.e. always explore. To avoid an\n",
    "    additional call to numpy random function, the choose method is re-implemented\n",
    "    as the copy of EpsilonGreedyPolicy without the condition evaluation.\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return 'random'\n",
    "    \n",
    "    def choose(self, agent):\n",
    "        return np.random.choice(len(agent.value_estimates))\n",
    "    \n",
    "class SoftmaxPolicy(Policy):\n",
    "    \"\"\"\n",
    "    The Softmax policy converts the estimated arm rewards into probabilities\n",
    "    then randomly samples from the resultant distribution. This policy is\n",
    "    primarily employed by the Gradient Agent for learning relative preferences.\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return 'SM'\n",
    "\n",
    "    def choose(self, agent):\n",
    "        a = agent.value_estimates\n",
    "        pi = np.exp(a) / np.sum(np.exp(a))\n",
    "        cdf = np.cumsum(pi)\n",
    "        s = np.random.random()\n",
    "        return np.where(s < cdf)[0][0]\n",
    "    \n",
    "    \n",
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    An Agent is able to take one of a set of actions at each time step. The\n",
    "    action is chosen using a strategy based on the history of prior actions\n",
    "    and outcome observations.\n",
    "    \"\"\"\n",
    "    def __init__(self, bandit, policy, prior=0, gamma=None):\n",
    "        self.policy = policy\n",
    "        self.k = bandit.k\n",
    "        self.bandit = bandit\n",
    "        self.prior = prior\n",
    "        self.gamma = gamma\n",
    "        self._value_estimates = prior*np.ones(self.k)\n",
    "        self.action_attempts = np.zeros(self.k)\n",
    "        self.t = 0\n",
    "        self.last_action = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return '{}'.format(str(self.policy))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the agent's memory to an initial state.\n",
    "        \"\"\"\n",
    "        self._value_estimates[:] = self.prior\n",
    "        self.action_attempts[:] = 0\n",
    "        self.last_action = None\n",
    "        self.t = 0\n",
    "        self.bandit.reset()\n",
    "\n",
    "    def choose(self):\n",
    "        action = self.policy.choose(self)\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def observe(self, reward):\n",
    "        self.action_attempts[self.last_action] += 1\n",
    "\n",
    "        if self.gamma is None:\n",
    "            g = 1 / self.action_attempts[self.last_action]\n",
    "        else:\n",
    "            g = self.gamma\n",
    "        q = self._value_estimates[self.last_action]\n",
    "\n",
    "        self._value_estimates[self.last_action] += g*(reward - q)\n",
    "        self.t += 1\n",
    "        \n",
    "    def obsrv(self, reward):\n",
    "        self.action_attempts[self.last_action] += 1\n",
    "        self._value_estimates[self.last_action] = (self._value_estimates[self.last_action] + reward) / 2\n",
    "        self.t += 1\n",
    "\n",
    "    @property\n",
    "    def value_estimates(self):\n",
    "        return self._value_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runner\n",
    "Definition of the class in charge of running the experiment. Defines a condition to stop the experiment and creates an instance of the required elements to run the simulation. It also defines the number of one-armed bandits that will be used in the simulation. In order to obtain meaningful results, the simulator has to run several times each experiment, obtaining an average of the results to work with. Thus, a parameter for the number of experiments is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class Simulator(object):\n",
    "    def __init__(self, agents, iterations, experiments):\n",
    "        self.agents = agents\n",
    "        self.iterations = iterations\n",
    "        self.experiments = experiments\n",
    "    \n",
    "    def reset(self):\n",
    "        for agent in self.agents:\n",
    "            agent.reset()\n",
    "    \n",
    "    def run(self):\n",
    "        self.scores = np.zeros((self.iterations, len(self.agents)))\n",
    "        self.optimal = np.zeros_like(self.scores)\n",
    "        # Assuming that all the bandits in the experiment have the same arms distribution\n",
    "        n_agents = len(self.agents)\n",
    "        n_arms = self.agents[0].bandit.k\n",
    "        self.arms_selection_count = np.zeros(n_arms)\n",
    "        self.arms_selection = []\n",
    "        \n",
    "        for xp in range(self.experiments):\n",
    "            self.reset()\n",
    "            list_arms_selected = []\n",
    "            for t in range(self.iterations):\n",
    "                for i, agent in enumerate(self.agents):\n",
    "                    action = agent.choose()\n",
    "                    reward, is_optimal = agent.bandit.pull(action)\n",
    "                    #agent.observe(reward)\n",
    "                    agent.obsrv(reward)\n",
    "\n",
    "                    self.scores[t, i] += reward\n",
    "                    list_arms_selected.append((action, reward))\n",
    "                    if is_optimal:\n",
    "                        self.optimal[t, i] += 1\n",
    "                    self.arms_selection_count[action] += 1\n",
    "            self.arms_selection.append(list_arms_selected)\n",
    "        \n",
    "        self.arms_selection_count = self.arms_selection_count / (self.experiments * n_agents)\n",
    "        self.scores = self.scores / self.experiments\n",
    "        self.optimal = self.optimal / self.experiments\n",
    "    \n",
    "    def run2(self):\n",
    "        self.scores = np.zeros((self.iterations, len(self.agents)))\n",
    "        self.optimal = np.zeros_like(self.scores)\n",
    "        # Assuming that all the bandits in the experiment have the same arms distribution\n",
    "        n_agents = len(self.agents)\n",
    "        n_arms = self.agents[0].bandit.k\n",
    "        self.arms_selection_count = np.zeros(n_arms)\n",
    "        self.arms_selection = []\n",
    "        \n",
    "        for i, agent in enumerate(self.agents):\n",
    "            self.reset()\n",
    "            arms_selection_count_l = np.zeros(n_arms)\n",
    "            for xp in range(self.experiments):\n",
    "                list_arms_selected = []\n",
    "                for t in range(self.iterations):\n",
    "                    action = agent.choose()\n",
    "                    reward, is_optimal = agent.bandit.pull(action)\n",
    "                    #agent.observe(reward)\n",
    "                    agent.obsrv(reward)\n",
    "\n",
    "                    self.scores[t, i] += reward\n",
    "                    list_arms_selected.append((action, reward))\n",
    "                    if is_optimal:\n",
    "                        self.optimal[t, i] += 1\n",
    "                    self.arms_selection_count[action] += 1\n",
    "                    arms_selection_count_l[action] += 1\n",
    "                if xp == self.experiments - 1:\n",
    "                    self.arms_selection.append(list_arms_selected)\n",
    "            \n",
    "            self.plot_arm_selection_histogram(arms_selection_count_l / self.experiments, str(agent))\n",
    "        \n",
    "        self.arms_selection_count = self.arms_selection_count / (self.experiments * n_agents)\n",
    "        self.scores = self.scores / self.experiments\n",
    "        self.optimal = self.optimal / self.experiments\n",
    "    \n",
    "    def plot_arm_selection_histogram_global(self):\n",
    "        plt.subplot(1,1,1)\n",
    "        sns.set_style('white')\n",
    "        sns.set_context('talk')\n",
    "        plt.title('Arm selection histogram')\n",
    "        \n",
    "        x_pos = np.arange(len(self.arms_selection_count))\n",
    "        vals = self.arms_selection_count\n",
    "        plt.bar(x_pos, vals,align='center')\n",
    "        plt.xticks(x_pos, np.arange(1, len(self.arms_selection_count) + 1)) \n",
    "        sns.despine()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_arm_selection_histogram(self, arms_selection_count, agent_label):\n",
    "        plt.subplot(1,1,1)\n",
    "        sns.set_style('white')\n",
    "        sns.set_context('talk')\n",
    "        plt.title('Arm selection histogram - ' + agent_label)\n",
    "        \n",
    "        x_pos = np.arange(len(arms_selection_count))\n",
    "        vals = arms_selection_count\n",
    "        \n",
    "        plt.bar(x_pos, vals,align='center')\n",
    "        plt.xticks(x_pos, np.arange(1, len(arms_selection_count) + 1)) \n",
    "        sns.despine()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_agents_avg_reward(self):\n",
    "        sns.set_style('white')\n",
    "        sns.set_context('talk')\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.title('Results')\n",
    "        plt.plot(self.scores)\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.legend(self.agents, loc=4)\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.legend(self.agents, loc=4)\n",
    "        sns.despine()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_arm_rewards(self):\n",
    "        iterations = []\n",
    "        arm_values = []\n",
    "        num_arms = self.agents[0].bandit.k\n",
    "        \n",
    "        for _ in range(num_arms):\n",
    "            arm_values.append([])\n",
    "            iterations.append([])\n",
    "        \n",
    "        for exp_vals in self.arms_selection:\n",
    "            count = 0\n",
    "            for iter_val in exp_vals:\n",
    "                iterations[iter_val[0]].append(count % self.iterations + 1)\n",
    "                arm_values[iter_val[0]].append(iter_val[1])\n",
    "                count += 1\n",
    "        \n",
    "        for arm in range(num_arms):\n",
    "            self.plot_single_arm_reward(iterations[arm], arm_values[arm], self.agents[0].bandit.arms[arm].mu, str(arm+1))\n",
    "    \n",
    "    def plot_single_arm_reward(self, iterations, arm_values, arm_mean, arm_label):\n",
    "        sns.set_style('white')\n",
    "        sns.set_context('talk')\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.plot(iterations, arm_values, 'ro')\n",
    "        plt.axhline(y=arm_mean, xmin=0, xmax=1, hold=None)\n",
    "        plt.xlim([0, self.iterations + 1])\n",
    "        plt.xlabel('Time Step')\n",
    "        sns.despine()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAETCAYAAAA1Rb1FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd17a577b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "arms = [GaussianArm(2.3, 0.9),\n",
    "        GaussianArm(2.1, 0.6),\n",
    "        GaussianArm(1.5, 0.4),\n",
    "        GaussianArm(1.3, 2.0)]\n",
    "'''\n",
    "\n",
    "arms = [GaussianArm2(2.3, 0.9),\n",
    "        GaussianArm2(2.1, 0.6),\n",
    "        GaussianArm2(1.5, 0.4),\n",
    "        GaussianArm2(1.3, 2.0)]\n",
    "'''\n",
    "\n",
    "agents = [Agent(MultiArmedBanditt(arms), RandomPolicy()),\n",
    "          Agent(MultiArmedBanditt(copy.deepcopy(arms)), EpsilonGreedyPolicy(0.0)),\n",
    "          Agent(MultiArmedBanditt(copy.deepcopy(arms)), EpsilonGreedyPolicy(0.1)),\n",
    "          Agent(MultiArmedBanditt(copy.deepcopy(arms)), EpsilonGreedyPolicy(0.2))]\n",
    "#agents[3] = Agent(MultiArmedBanditt(arms), SoftmaxPolicy(1.0))\n",
    "#agents[4] = Agent(MultiArmedBanditt(arms), SoftmaxPolicy(0.1))\n",
    "\n",
    "iterations = 1000\n",
    "experiments = 10000\n",
    "\n",
    "simulator = Simulator(agents, iterations, experiments)\n",
    "simulator.run2()\n",
    "simulator.plot_arm_selection_histogram_global()\n",
    "simulator.plot_agents_avg_reward()\n",
    "simulator.plot_arm_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
