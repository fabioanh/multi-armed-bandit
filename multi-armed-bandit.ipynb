{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Classes\n",
    "Definition of basic classes required for the solution of the multi-armed bandit problem. Inspiration and code used from https://github.com/bgalbraith/bandits. The problem explanation can be better follwed in the chapter 2 of the book _“Reinforcement Learning, An Introduction”_ by Sutton and Barto  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspired by https://github.com/bgalbraith/bandits\n",
    "# Some part of the code were taken from this repository \n",
    "# and modified according to requirements of the current project.\n",
    "# License of the project adjusted to be compliant with the used source.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MultiArmedBandit(object):\n",
    "    \"\"\"\n",
    "    A Multi-armed Bandit\n",
    "    \"\"\"\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.action_values = np.zeros(k)\n",
    "        self.optimal = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.action_values = np.zeros(self.k)\n",
    "        self.optimal = 0\n",
    "\n",
    "    def pull(self, action):\n",
    "        return 0, True\n",
    "\n",
    "\n",
    "class GaussianBandit(MultiArmedBandit):\n",
    "    \"\"\"\n",
    "    Gaussian bandits model the reward of a given arm as normal distribution with\n",
    "    provided mean and standard deviation.\n",
    "    \"\"\"\n",
    "    def __init__(self, k, mu=0, sigma=1):\n",
    "        super(GaussianBandit, self).__init__(k)\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.action_values = np.random.normal(self.mu, self.sigma, self.k)\n",
    "        self.optimal = np.argmax(self.action_values)\n",
    "\n",
    "    def pull(self, action):\n",
    "        return (np.random.normal(self.action_values[action]),\n",
    "                action == self.optimal)\n",
    "\n",
    "class Policy(object):\n",
    "    \"\"\"\n",
    "    A policy prescribes an action to be taken based on the memory of an agent.\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return 'generic policy'\n",
    "\n",
    "    def choose(self, agent):\n",
    "        return 0\n",
    "\n",
    "\n",
    "class EpsilonGreedyPolicy(Policy):\n",
    "    \"\"\"\n",
    "    The Epsilon-Greedy policy will choose a random action with probability\n",
    "    epsilon and take the best apparent approach with probability 1-epsilon. If\n",
    "    multiple actions are tied for best choice, then a random action from that\n",
    "    subset is selected.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\u03B5-greedy (\\u03B5={})'.format(self.epsilon)\n",
    "\n",
    "    def choose(self, agent):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.choice(len(agent.value_estimates))\n",
    "        else:\n",
    "            action = np.argmax(agent.value_estimates)\n",
    "            check = np.where(agent.value_estimates == action)[0]\n",
    "            if len(check) == 0:\n",
    "                return action\n",
    "            else:\n",
    "                return np.random.choice(check)\n",
    "\n",
    "class RandomPolicy(EpsilonGreedyPolicy):\n",
    "    \"\"\"\n",
    "    The Random policy randomly selects from all available actions with no\n",
    "    consideration to which is apparently best. This can be seen as a special\n",
    "    case of EpsilonGreedy where epsilon = 1 i.e. always explore.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(RandomPolicy, self).__init__(1)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'random'\n",
    "\n",
    "class SoftmaxPolicy(Policy):\n",
    "    \"\"\"\n",
    "    The Softmax policy converts the estimated arm rewards into probabilities\n",
    "    then randomly samples from the resultant distribution. This policy is\n",
    "    primarily employed by the Gradient Agent for learning relative preferences.\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return 'SM'\n",
    "\n",
    "    def choose(self, agent):\n",
    "        a = agent.value_estimates\n",
    "        pi = np.exp(a) / np.sum(np.exp(a))\n",
    "        cdf = np.cumsum(pi)\n",
    "        s = np.random.random()\n",
    "        return np.where(s < cdf)[0][0]\n",
    "    \n",
    "    \n",
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    An Agent is able to take one of a set of actions at each time step. The\n",
    "    action is chosen using a strategy based on the history of prior actions\n",
    "    and outcome observations.\n",
    "    \"\"\"\n",
    "    def __init__(self, bandit, policy, prior=0, gamma=None):\n",
    "        self.policy = policy\n",
    "        self.k = bandit.k\n",
    "        self.prior = prior\n",
    "        self.gamma = gamma\n",
    "        self._value_estimates = prior*np.ones(self.k)\n",
    "        self.action_attempts = np.zeros(self.k)\n",
    "        self.t = 0\n",
    "        self.last_action = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'f/{}'.format(str(self.policy))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the agent's memory to an initial state.\n",
    "        \"\"\"\n",
    "        self._value_estimates[:] = self.prior\n",
    "        self.action_attempts[:] = 0\n",
    "        self.last_action = None\n",
    "        self.t = 0\n",
    "\n",
    "    def choose(self):\n",
    "        action = self.policy.choose(self)\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def observe(self, reward):\n",
    "        self.action_attempts[self.last_action] += 1\n",
    "\n",
    "        if self.gamma is None:\n",
    "            g = 1 / self.action_attempts[self.last_action]\n",
    "        else:\n",
    "            g = self.gamma\n",
    "        q = self._value_estimates[self.last_action]\n",
    "\n",
    "        self._value_estimates[self.last_action] += g*(reward - q)\n",
    "        self.t += 1\n",
    "\n",
    "    @property\n",
    "    def value_estimates(self):\n",
    "        return self._value_estimates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
